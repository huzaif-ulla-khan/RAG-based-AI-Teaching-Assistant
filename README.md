# ğŸ“ RAG-Based AI Teaching Assistant

A **Retrieval-Augmented Generation (RAG)** based AI Teaching Assistant that allows students to ask natural language questions and receive **grounded, timestamp-accurate answers** directly from course lecture videos.

Instead of relying on generic AI knowledge, this system retrieves relevant video transcript chunks, injects them into a carefully constrained prompt, and generates answers that reference **exact videos and timestamps** â€” making learning faster, clearer, and more reliable.

---

## âœ¨ Key Features

- ğŸ¥ **Video-first learning** â€“ Answers are grounded in actual lecture videos  
- â±ï¸ **Timestamp-accurate guidance** â€“ Points students to the exact moment a concept is taught  
- ğŸ” **Semantic search with embeddings** â€“ Finds relevant content using vector similarity  
- ğŸ§  **RAG architecture** â€“ Eliminates hallucinations by answering strictly from retrieved context  
- ğŸ§© **Modular pipeline** â€“ Easy to adapt for any course or video dataset  
- ğŸ’» **Flexible deployment** â€“ Runs locally with lightweight models and supports cloud LLMs  

---

## ğŸ—ï¸ Project Architecture

```
Videos
  â†“
Audio Extraction (ffmpeg)
  â†“
Speech-to-Text (Whisper)
  â†“
Chunked Transcripts (JSON)
  â†“
Embeddings (Vectorization)
  â†“
Vector Similarity Search
  â†“
Prompt Construction (RAG)
  â†“
LLM Answer with Video + Timestamp
```

---

## ğŸ“‚ Project Structure

```
RAG-based-ai/
â”‚
â”œâ”€â”€ videos/                  # Raw course videos (not included due to size limits)
â”œâ”€â”€ audios/                  # Extracted mp3 audio files
â”œâ”€â”€ jsons/                   # Timestamped transcript chunks
â”‚
â”œâ”€â”€ video_to_mp3.py          # Converts videos â†’ mp3 using ffmpeg
â”œâ”€â”€ mp3_to_json.py           # Transcribes mp3 â†’ JSON using Whisper
â”œâ”€â”€ preprocess_json.py       # Generates embeddings & saves vector store
â”œâ”€â”€ process_incoming.py      # Query â†’ retrieval â†’ prompt â†’ LLM answer
â”‚
â”œâ”€â”€ embeddings.joblib        # Stored vector embeddings
â”œâ”€â”€ prompt.txt               # Generated RAG prompt (for inspection)
â”œâ”€â”€ response.txt             # Model response output
â””â”€â”€ README.md                # Project documentation
```

---

## âš™ï¸ How It Works (Step-by-Step)

### 1ï¸âƒ£ Convert Videos to Audio

All lecture videos are converted to `.mp3` format for transcription.

```bash
python video_to_mp3.py
```

Uses `ffmpeg` to extract audio while preserving video numbering and titles.

### 2ï¸âƒ£ Transcribe Audio to Timestamped JSON

Each audio file is transcribed using OpenAI Whisper, producing timestamped subtitle chunks.

```bash
python mp3_to_json.py
```

Each chunk contains:
1. Video number
2. Video title
3. Start time
4. End time
5. Spoken text

### 3ï¸âƒ£ Generate Embeddings (Vectorization)

All transcript chunks are converted into embeddings using Ollama's embedding API.

```bash
python preprocess_json.py
```

- Embeddings are saved as `embeddings.joblib`
- Enables fast semantic similarity search

### 4ï¸âƒ£ Ask Questions (RAG Inference)

Students can ask questions in natural language.

```bash
python process_incoming.py
```

The system:
- Embeds the user question
- Retrieves top-K relevant transcript chunks
- Builds a strict RAG prompt
- Sends it to a lightweight LLM
- Returns a grounded answer with video number & timestamps

---

## ğŸ§  Prompt Design (Anti-Hallucination)

The model is explicitly restricted to answer only from retrieved transcript chunks.

If the answer is not present in the videos, the system responds:

> "I could not find this topic clearly explained in the provided videos."

This ensures trustworthy, course-aligned answers.

---

## ğŸ–¥ï¸ Models Used

### ğŸ™ï¸ Speech-to-Text
**Whisper (base)** â€“ stable and CPU-friendly

### ğŸ” Embeddings
**nomic-embed-text** (via Ollama)
- 768-dimensional embeddings
- Optimized for semantic search

### ğŸ¤– Language Model
**qwen2.5:1.5b** â€“ lightweight and local-friendly
- Easy to swap with cloud LLMs (OpenAI, Groq, etc.)

---

## âš ï¸ Hardware Notes

Designed to work on low-resource systems. Local LLMs are intentionally lightweight.

For best performance:
- Use cloud LLM APIs for inference
- Keep embeddings and retrieval local

---

## ğŸ§ª Example Query

**Question:**  
Where were semantic tags taught?

**Answer:**  
**Video 11:** Installing VS Code & How Websites Work  
**Timestamp:** 109.82 â€“ 145.30 seconds  
**Explanation:** This section introduces semantic HTML tags, explains their purpose, and why they improve structure and accessibility.

---

## ğŸš€ Use Cases

- AI tutor for recorded courses
- Timestamp-based doubt resolution
- Self-paced learning assistant
- EdTech platforms
- Internal corporate training
- Lecture search & summarization

---

## ğŸ”® Future Improvements

- ğŸŒ Web UI (FastAPI + Frontend)
- ğŸ”— Clickable video timestamps
- ğŸ“Š Confidence scoring for answers
- ğŸ§  Personalized learning paths
- â˜ï¸ Cloud GPU deployment
- ğŸ“š Multi-course support

---

## ğŸ‘¤ Author

**Huzaif Ulla Khan**  
BE in Computer Science Engineering  
AI & Machine Learning Enthusiast

**Project:** RAG-Based AI Teaching Assistant
